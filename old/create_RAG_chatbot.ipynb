{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98d1bc9-a7e9-4f91-8a50-c0ffb04644a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a000a275-7371-4ab4-b7be-abddc1f67ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cda4c3e-b40c-4cb6-8498-1172048f3324",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_databricks.vectorstores import DatabricksVectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4381a4cc-f6a2-4953-8007-1c6af16ac83b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Write the chain to a .py file to avoid serialization issues\n",
    "When working with LangChain and complex LLM-based workflows, you might need to write the LLM chain to a companion file to avoid serialization issues for several reasons:\n",
    "1. Serialization limitations: Some LLM chain components, particularly custom tools or complex objects, may not be directly JSON serializable. This can cause errors when trying to save or transmit the entire chain as a single object.\n",
    "2. Preservation of functionality: Certain elements of an LLM chain, such as custom functions or methods, cannot be fully serialized without losing their functionality. Writing these components separately allows you to maintain their integrity.\n",
    "3. Flexibility and modularity: By separating the LLM chain definition from its serialized state, you can more easily update or modify parts of the chain without affecting the entire workflow.\n",
    "4. Compatibility across environments: Serialization issues can arise when moving LangChain objects between different environments or platforms. Writing the chain separately ensures better compatibility and portability.\n",
    "5. Security considerations: Serializing entire LLM chains might inadvertently expose sensitive information or credentials. Separating the chain definition allows for better control over what is serialized and transmitted.\n",
    "6. Performance optimization: For large or complex chains, serializing only the necessary components can improve performance and reduce memory usage.\n",
    "7. Version control and maintenance: Keeping the LLM chain definition in a separate file makes it easier to track changes, manage versions, and maintain the code over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6e9e14-297c-495a-b24d-bd4a5ef809a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain.py\n",
    "import os\n",
    "import mlflow\n",
    "from operator import itemgetter\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def extract_previous_messages(chat_messages_array):\n",
    "    messages = \"\\n\"\n",
    "    for msg in chat_messages_array[:-1]:\n",
    "        messages += (msg[\"role\"] + \": \" + msg[\"content\"] + \"\\n\")\n",
    "    return messages\n",
    "\n",
    "def combine_all_messages_for_vector_search(chat_messages_array):\n",
    "    return extract_previous_messages(chat_messages_array) + extract_user_query_string(chat_messages_array)\n",
    "\n",
    "#Get the conf from the local conf file\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_model_config.yaml')\n",
    "\n",
    "databricks_resources = model_config.get(\"databricks_resources\")\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "llm_config = model_config.get(\"llm_config\")\n",
    "\n",
    "# Connect to the Vector Search Index\n",
    "vs_client = VectorSearchClient(disable_notice=True)\n",
    "vs_index = vs_client.get_index(\n",
    "    endpoint_name=databricks_resources.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=retriever_config.get(\"vector_search_index\"),\n",
    ")\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    columns=[\n",
    "        vector_search_schema.get(\"primary_key\"),\n",
    "        vector_search_schema.get(\"chunk_text\")#,\n",
    "##        vector_search_schema.get(\"document_uri\"),\n",
    "    ],\n",
    ").as_retriever(search_kwargs=retriever_config.get(\"parameters\"))\n",
    "\n",
    "# Required to:\n",
    "# 1. Enable the RAG Studio Review App to properly display retrieved chunks\n",
    "# 2. Enable evaluation suite to measure the retriever\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=vector_search_schema.get(\"primary_key\"),\n",
    "    text_column=vector_search_schema.get(\"chunk_text\") #,\n",
    "#    doc_uri=vector_search_schema.get(\"document_uri\")\n",
    ")\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt\n",
    "def format_context(docs):\n",
    "    chunk_template = retriever_config.get(\"chunk_template\")\n",
    "    chunk_contents = [\n",
    "        chunk_template.format(\n",
    "            chunk_text=d.page_content,\n",
    "        )\n",
    "        for d in docs\n",
    "    ]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "# Prompt Template for generation\n",
    "prompt = PromptTemplate(\n",
    "    template=llm_config.get(\"llm_prompt_template\"),\n",
    "    input_variables=llm_config.get(\"llm_prompt_template_variables\"),\n",
    ")\n",
    "\n",
    "# Foundation Models for generation\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"context\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(combine_all_messages_for_vector_search)\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_previous_messages)\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56c4474-8465-4db9-a753-2c70398dc891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=f\"call_transcript_rag\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "        model_config='rag_model_config.yaml',  # Chain configuration \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=model_config.get(\"input_example\"),  # Save the chain's input schema.  MLflow will execute the chain before logging & capture it's output schema.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0016122-9ac9-4aa3-b727-00b3e6acfde9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the chain locally\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(model_config.get(\"input_example\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_RAG_chatbot",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
